## 量化技术

### PTQ（后训练量化）=不重训，直接压

- 准备一小把“样本”（几百条真实输入），帮它调一调“刻度”。
- 好处：快、省事；坏处：特别低比特（比如INT4）时，准确率可能掉一点。

### QAT（量化感知训练）=带点微调再压

- 在训练/微调时让模型“提前适应”粗精度。
- 好处：效果更稳；坏处：要花点训练资源。

### KV Cache

**“KV Cache INT8/FP16 混合”** 是大语言模型（LLM，例如 GPT、Qwen、LLaMA 等）部署和加速里非常关键的优化点。
🧠 一、什么是 KV Cache？

在 Transformer 的 **自注意力（Self-Attention）** 机制里，每个 token（词）都会生成三种向量：

- **K**（Key）
- **V**（Value）
- **Q**（Query）

注意力计算公式是：

Attention(Q,K,V)=softmax(QKT/d)V\text{Attention}(Q, K, V) = \text{softmax}(QK^T / \sqrt{d})V

Attention(Q,K,V)=softmax(QKT/d

)V

🚀 推理时的问题

---

在 LLM 推理（inference）中，模型是**逐字生成**的：

- 第 1 次：输入第一个 token，计算出它的 K、V。
- 第 2 次：输入下一个 token，仍然需要用到**之前所有 token 的 K、V**。
- 第 3 次：又要用到前面所有的 K、V……

👉 所以，为了避免每次都重新计算所有历史的 K、V，我们会 **把每一步算出来的 K、V 存起来** —— 这块缓存就叫：

> KV Cache（Key-Value Cache）
> 

🧩 形象比喻

---

你可以把它理解成：

> 模型在生成长文本时，KV Cache 就像“记忆本”，记录之前每个词的语义特征，方便后面直接查阅，不用重复思考。
> 

⚙️ 二、为什么要对 KV Cache 量化？

---

因为在长文本生成中，KV Cache 体积非常大！

举个例子：

- 模型有 **70 亿参数（7B）**；
- 序列长度 4K；
- 每个 token 的 K/V 占用约几十 KB；
- 一次推理可能要缓存几 GB 的 KV Cache！

所以：

- **FP16（16 位浮点）** 存太大；
- **INT8（8 位整数）** 更省显存；
- 于是就出现了 **“KV Cache INT8/FP16 混合”** 的方案。

⚖️ 三、KV Cache INT8 / FP16 混合的含义

---

| 部分 | 精度 | 说明 |
| --- | --- | --- |
| **计算部分（QK^T、Softmax 等）** | FP16 | 保留高精度，确保推理稳定性 |
| **缓存部分（K、V存储）** | INT8 | 只存储为 8 位整数，节省显存和带宽 |
| **恢复时（取出使用）** | 解量化为 FP16 | 在计算前恢复为高精度 |

也就是说：

> 模型在计算时仍用 FP16 做注意力运算，但在显存中只保存 INT8 的压缩版 K、V，从而 减少 50% 显存占用、提高带宽利用率。
> 

🧮 四、效果与应用

---

| 模式 | KV Cache 占用 | 速度 | 精度影响 |
| --- | --- | --- | --- |
| FP16 | 100% | 标准速度 | 无 |
| INT8 | ~50% | 更快 | 微小（<0.1% perplexity drop） |
| 混合 (INT8存储 + FP16计算) | 约 55–60% | 加速明显 | 几乎无感 |

这种混合方案现在在：

- **vLLM**、**AWQ**、**SmoothQuant**、**GPTQ**、**Qwen2.5** 等系统中都已广泛使用。

🧭 五、总结一句话

---

> KV Cache 是模型在生成过程中的“历史记忆”；
“KV Cache INT8/FP16 混合” 就是把这份记忆压缩保存为 INT8，以减少显存占用，同时在用的时候临时恢复为 FP16 来保证精度。
> 

### W8A8

“**W8A8**” 是在模型**量化（quantization）**里非常常见的一种简写形式，意思是：

> W = Weights（权重）8-bit，A = Activations（激活）8-bit
> 

也就是把模型中的**权重参数和激活值**都从原来的高精度（比如 FP32 浮点数）**量化为 8 位整数（INT8）** 来计算。

🔍 详细解释

---

| 名称 | 含义 | 说明 |
| --- | --- | --- |
| **W8** | 权重量化为 8bit | 每个权重值用 8 位整数存储（而不是 32 位浮点数） |
| **A8** | 激活量化为 8bit | 每个神经元输出（中间特征）也用 8 位整数表示 |
| **W8A8 模型** | 整体是 8bit 整数推理模型 | 权重和激活都低精度化，显著减少显存和计算量 |

---

⚙️ 对比示例

| 模型精度 | 权重精度 | 激活精度 | 模型大小 | 计算速度 | 精度损失 |
| --- | --- | --- | --- | --- | --- |
| FP32 | 32 bit | 32 bit | 100% | 1× | 无损 |
| FP16 | 16 bit | 16 bit | 50% | 1.5–2× | 极小 |
| **INT8 (W8A8)** | 8 bit | 8 bit | **25%** | **2–4×** | 轻微 |
| INT4 (W4A4) | 4 bit | 4 bit | 12.5% | 4–8× | 可能较大 |

---

💡 应用场景

- 推理部署（如 Jetson、手机、NPU、BPU 等嵌入式设备）
- 大语言模型或视觉模型压缩（LLM.int8、YOLO INT8 部署）
- 工业框架中常见写法：
    - `W8A8`：全 8bit 量化
    - `W8A16`：权重 8bit，激活保持 16bit 精度
    - `W4A8`：混合量化，权重更低精度以进一步压缩模型

## 大模型/多模态部署

### 大语言模型（LLM）

- 把**权重压到8位**（W8A8 或“只压权重 W4/W8”）很常见，**显存减半左右**，速度提升明显；
- 一些“娇气”的地方（比如**第一层、最后一层、归一化、注意力里的Softmax/QK^T**）常保持高精度，避免变笨。
- *长对话记忆（KV Cache）**也能量化到INT8/FP8，显存压力大降。

### 多模态（图像/语音/视频+文本）

- 视觉主干/语音主干用INT8通常**效果OK**；
- **跨模态对齐/投影头**对质量很敏感，常保高精度或用QAT。
- 做检测/分类时，INT8一般**精度掉 <1%~2%**；INT4通常要QAT才稳。

## 小白无脑操作

- **先选8位**（INT8/FP8），成功率高、收益也大。
- **先做PTQ**：
    1. 导出模型（ONNX 等）。
    2. 准备**小而真实**的样本当“校准集”（比如你实际业务的几十～几百条输入）。
    3. 用推理引擎（如 TensorRT / ONNX Runtime / TFLite / CoreML）的一键量化流程跑一下。
    4. 测：速度、显存、准确率。
- **再优化**：
    - 哪些层掉点就**不量化**（保FP16），其余继续INT8 → **混合精度**。
    - 还想再省？再考虑**INT4**，但优先用**QAT微调**保证不掉精度。

### 一个超简模板

- **服务器跑LLM（NVIDIA GPU）**：W8A8 + 关键层FP16，KV Cache INT8/FP16混合；工具优先 TensorRT。
- **本地/显存紧张**：**只量化权重到4位（W4A16）**，效果常能接受，加载大模型更轻松。
- **边缘设备跑多模态**：视觉主干INT8，跨模态对齐头FP16；用TFLite/NNAPI/CoreML等官方路线。
