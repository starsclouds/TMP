### FP32：单精度浮点数（float32）

Floating Point 32-bit

位数：32位

- 1位符号位（正/负）
- 8位指数位（表示数值范围）
- 23位尾数位（表示小数精度）

特点：精度最高、计算慢显存占用高

### FP16：半精度浮点数（float16）

Floating Point 16-bit

位数：16位

- 1位符号位（正/负）
- 5位指数位（表示数值范围）
- 10位尾数位（表示小数精度）

特点：现代GPU（A100、H800）对FP16有专门加速单元

用途：训练或推理常用“混合精度训练（FP16+FP32）”

### FP8：超低精度浮点数（float8）

Floating Point 8-bit

位数：8位

- 1位符号位（正/负）
- 2-5位指数位（表示数值范围）
- 2-3位尾数位（表示小数精度）

用途：最新的NVIDIA H100/H200 GPU支持FP8训练与推理，是当前量化趋势之一。

### INT8：8位整型数（integer 8-bit）(W8A8)

Integer 8-bit（即只有整数，没有小数）

范围：

- 有符号：-128~127
- 无符号：0~255

特点：

- 速度非常快（CPU/GPU/NPU都支持）
- 存储量仅FP32的1/4
- 需要**量化和反量化映射**（把浮点数映射到整数范围）

例子：原来某层权重是 0.234、-1.238 → 通过比例因子 scale，变成 30、-160 这样的整数。

W8A8
“**W8A8**” 是在模型**量化（quantization）**里非常常见的一种简写形式，意思是：
W = Weights（权重）8-bit，A = Activations（激活）8-bit
也就是把模型中的**权重参数和激活值**都从原来的高精度（比如 FP32 浮点数）**量化为 8 位整数（INT8）** 来计算。

### INT4:4位整型数（integer 4-bit）(W4A4)

Interger 4-bit

范围：-8~7（有符号）

特点：

- 常用于“模型部署”或“边缘设备”
- 精度损失较大，需要精细化的量化策略（如GPTQ、AWQ、SmoothQuant等）。

### 总结对比表：

| 格式 | 类型 | 位宽 | 速度 | 精度 | 典型用途 |
| --- | --- | --- | --- | --- | --- |
| FP32 | 浮点 | 32-bit | 慢 | ⭐⭐⭐⭐ | 训练主格式 |
| FP16 | 浮点 | 16-bit | 快 | ⭐⭐⭐ | 混合精度训练 |
| FP8 | 浮点 | 8-bit | 更快 | ⭐⭐ | 新一代GPU训练 |
| INT8 | 整数 | 8-bit | 很快 | ⭐⭐ | 量化推理 |
| INT4 | 整数 | 4-bit | 极快 | ⭐ | 超轻量推理（端侧） |
